\documentclass{article} % For initial submission. Use \documentclass[final]{article} for camera-ready
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}

% Listings settings for code
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt
}

\title{Copilot Agent Factory: Automated Generation of Context-Aware AI Coding Agents from Repository Structure}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Anonymous Authors \\
Affiliation \\
Address \\
\texttt{email@domain.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}

\maketitle

\begin{abstract}
Modern AI-powered coding assistants like GitHub Copilot, Claude Code, and Cursor IDE provide valuable development support, but often lack deep understanding of project-specific contexts such as tech stack, architecture, and coding conventions. We introduce \textbf{Copilot Agent Factory}, an automated system that generates context-aware AI coding agents by analyzing repository structure, detecting frameworks and patterns, and instantiating specialized agent templates. Our system features 41 specialized agent templates across 12 categories (planning, development, ML/AI, mobile, etc.), a detection-based template selection mechanism, and a placeholder resolution system that customizes agents with repository-specific commands and conventions. The system supports multiple platforms (VS Code/GitHub Copilot, Claude Code, Cursor IDE) and includes a structured workflow system with approval gates for controlled development. We demonstrate the effectiveness of our approach through empirical evaluation across diverse repositories, showing high detection accuracy (95\%+ precision) and successful generation of fully-functional, project-specific agents. Our work democratizes access to specialized AI coding assistance and provides a foundation for automated agent customization in software development.
\end{abstract}

\section{Introduction}

The landscape of software development has been transformed by AI-powered coding assistants such as GitHub Copilot \citep{github2021copilot}, Claude Code, and Cursor IDE. These tools leverage large language models to provide code suggestions, bug fixes, and development guidance. However, a fundamental limitation persists: \textbf{these assistants operate with generic knowledge and lack deep understanding of project-specific contexts}.

\subsection{Motivation}

Consider a developer working on a machine learning project using PyTorch with a specific directory structure, testing framework (pytest), and coding conventions. A generic AI assistant might suggest TensorFlow code, recommend incorrect testing approaches, or violate project-specific style guidelines. While the assistant possesses broad programming knowledge, it lacks awareness of:

\begin{itemize}
    \item \textbf{Tech stack}: Languages, frameworks, and libraries used in the project
    \item \textbf{Architecture patterns}: Microservices, monolith, or domain-driven design
    \item \textbf{Build and test commands}: Project-specific tooling and workflows
    \item \textbf{Coding conventions}: Style guides, naming patterns, and best practices
    \item \textbf{Directory structure}: Organization of source, test, and configuration files
\end{itemize}

Recent platforms like GitHub Copilot and Claude Code introduced the concept of \textit{custom agents}—specialized AI assistants with role-based expertise (e.g., ``test agent'', ``documentation agent''). However, creating these agents requires:
\begin{enumerate}
    \item Manual analysis of repository structure and patterns
    \item Expertise in prompt engineering and agent design
    \item Ongoing maintenance as projects evolve
    \item Duplication of effort across similar projects
\end{enumerate}

This creates a barrier to entry and limits adoption of specialized agents to expert users.

\subsection{Our Contribution}

We present \textbf{Copilot Agent Factory}, the first automated system for generating context-aware AI coding agents from repository structure. Our key contributions are:

\begin{enumerate}
    \item \textbf{Meta-template system}: A novel architecture featuring 41 specialized agent templates across 12 categories, designed to cover common development scenarios (planning, testing, debugging, ML training, mobile development, etc.).
    
    \item \textbf{Detection-based generation}: An automated pipeline that analyzes repositories to detect frameworks, patterns, and conventions, then selects and instantiates relevant agent templates.
    
    \item \textbf{Placeholder resolution}: A comprehensive system with 60+ placeholders that customizes agent instructions with repository-specific values (commands, directories, style guidelines).
    
    \item \textbf{Multi-platform support}: Output formats for VS Code (GitHub Copilot), Claude Code, and Cursor IDE, enabling broad adoption.
    
    \item \textbf{Structured workflows}: An orchestrator agent and approval-gate system for controlled development processes (PRD $\rightarrow$ Architecture $\rightarrow$ Design $\rightarrow$ Implementation $\rightarrow$ Review).
    
    \item \textbf{Empirical validation}: Evaluation across diverse repositories demonstrating 95\%+ detection precision and successful agent generation.
\end{enumerate}

Our approach transforms agent creation from a manual, expert-driven process into an automated, zero-configuration experience accessible to all developers.

\subsection{Paper Organization}

The remainder of this paper is organized as follows: Section~\ref{sec:related} reviews related work in AI coding assistants, program synthesis, and repository analysis. Section~\ref{sec:architecture} describes our system architecture. Section~\ref{sec:agents} details our agent design patterns and template system. Section~\ref{sec:workflow} presents the structured workflow system. Section~\ref{sec:implementation} covers implementation details. Section~\ref{sec:evaluation} provides empirical evaluation. Section~\ref{sec:discussion} discusses strengths, limitations, and future directions. Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\subsection{AI Coding Assistants}

AI-powered coding assistants have evolved rapidly in recent years. GitHub Copilot \citep{github2021copilot} pioneered the use of large language models for in-IDE code completion, trained on billions of lines of public code. Amazon CodeWhisperer \citep{amazonCodeWhisperer} provides similar functionality with additional security scanning. More recently, Claude Code (Anthropic) and Cursor IDE introduced conversational interfaces and custom agent capabilities.

These tools demonstrate impressive capabilities in code generation, bug fixing, and documentation. However, they operate primarily with generic knowledge. While some systems support custom instructions, these require manual specification and lack automated repository analysis.

\subsection{Code Generation and Program Synthesis}

Template-based code generation has a long history in software engineering \citep{czarnecki2000generative}. Systems like CodeGen \citep{nijkamp2023codegen} and AlphaCode \citep{li2022alphacode} focus on generating code from natural language descriptions. Program synthesis approaches \citep{gulwani2017program} generate programs from specifications.

Our work differs by focusing on \textit{agent generation} rather than code generation. We generate specialized AI assistants that understand project context, not just code snippets.

\subsection{Repository Mining and Analysis}

Automated repository analysis has been extensively studied for software engineering tasks. Tools like Boa \citep{dyer2013boa} enable large-scale mining of source code repositories. Framework detection approaches \citep{bavota2013mining} identify technologies used in projects. Dependency analysis tools \citep{dietrich2019dependency} extract library usage patterns.

Our detection engine builds on these concepts but focuses specifically on extracting information relevant for agent customization, including commands, conventions, and architectural patterns.

\subsection{Prompt Engineering and Agent Design}

Recent work on prompt engineering \citep{white2023prompt} has established best practices for crafting effective LLM instructions. Chain-of-thought prompting \citep{wei2023chainofthought} and role-based prompting have shown improvements in task performance.

Multi-agent systems \citep{xi2023multiagent} demonstrate benefits of specialized agents for complex tasks. AutoGPT and similar systems \citep{autogpt2023} explore autonomous agent behaviors.

Our contribution complements this work by providing a systematic approach to generating specialized agents tailored to specific development contexts.

\section{System Architecture}
\label{sec:architecture}

Copilot Agent Factory consists of three main components: (1) a \textit{Detection Engine} that analyzes repositories, (2) a \textit{Template System} containing specialized agent and skill templates, and (3) a \textit{Generation Pipeline} that produces customized agents. Figure~\ref{fig:architecture} illustrates the overall architecture.

\subsection{Detection Engine}

The Detection Engine performs multi-level repository analysis to identify relevant patterns:

\textbf{File-based detection} identifies technology stacks through characteristic files:
\begin{lstlisting}[language=bash]
package.json + "react" -> React framework
requirements.txt + "torch" -> PyTorch ML project
Cargo.toml -> Rust project
\end{lstlisting}

\textbf{Dependency analysis} extracts libraries and frameworks from package manifests (package.json, requirements.txt, pom.xml, etc.), enabling framework-specific agent selection.

\textbf{Configuration parsing} extracts commands from project configurations:
\begin{lstlisting}[language=json]
{
  "scripts": {
    "test": "jest --coverage",
    "build": "webpack --mode production"
  }
}
\end{lstlisting}

\textbf{Pattern matching} uses regular expressions and AST analysis to detect architectural patterns, coding conventions, and directory structures.

The detection process outputs a \textit{project profile} containing:
\begin{itemize}
    \item Tech stack (languages, frameworks, libraries)
    \item Build/test/lint commands
    \item Directory structure (source, test, docs, config)
    \item Architectural patterns (microservices, MVC, etc.)
    \item Coding conventions (style guides, naming patterns)
\end{itemize}

\subsection{Template System}

Our template system consists of two complementary types:

\subsubsection{Agent Templates}

Agents are \textit{role-based experts} that provide domain-specific guidance. Each agent template follows a standardized structure:

\begin{lstlisting}[language=yaml]
---
name: test-agent
model: claude-4-5-sonnet
description: Testing specialist for writing 
             and maintaining tests
triggers:
  - Test files detected
  - Testing framework present
---

You are an expert testing engineer for 
this project.

## Your Role
- Write comprehensive test cases
- Maintain test infrastructure
- Debug failing tests

## Project Knowledge
- Tech Stack: {{tech_stack}}
- Test Framework: {{test_framework}}
- Test Command: {{test_command}}
- Test Directories: {{test_dirs}}

## Commands
- Run tests: `{{test_command}}`
- Run with coverage: `{{coverage_command}}`

## Boundaries
- Always: Write tests for new code
- Ask First: Modify existing passing tests
- Never: Skip edge cases
\end{lstlisting}

We provide 41 specialized agent templates across 12 categories (detailed in Section~\ref{sec:agents}).

\subsubsection{Skill Templates}

Skills are \textit{procedural workflows} that auto-activate based on user queries. Unlike agents, skills provide step-by-step instructions with fallback logic:

\begin{lstlisting}[language=yaml]
---
name: run-tests-with-coverage
description: Run project tests with coverage
auto-activates:
  - "run tests with coverage"
  - "check test coverage"
---

## Step 1: Detect Test Command
If {{test_command}} is configured:
  - Run: `{{test_command}} --coverage`

If not configured:
  - For Python: `pytest --cov`
  - For JavaScript: `npm test -- --coverage`
  - For Go: `go test -cover ./...`

## Step 2: Generate Coverage Report
...
\end{lstlisting}

Skills use minimal placeholders (10 core placeholders) and provide fallback instructions for unconfigured environments.

\subsection{Generation Pipeline}

The generation pipeline orchestrates the transformation from repository to agents:

\begin{enumerate}
    \item \textbf{Analysis Phase}: Detection Engine produces project profile
    \item \textbf{Selection Phase}: Template selector chooses relevant agents based on detection triggers
    \item \textbf{Instantiation Phase}: Placeholder resolver replaces {{placeholders}} with project-specific values
    \item \textbf{Output Phase}: Agents written to platform-specific locations (.github/agents/, .claude/agents/, .cursor/agents/)
    \item \textbf{Format Conversion}: For Cursor IDE, convert .md to .mdc format
\end{enumerate}

The entire process is zero-configuration—developers simply invoke the agent-generator in their repository.

\section{Agent Design Patterns}
\label{sec:agents}

Our template library reflects careful design choices informed by software development practices and AI agent capabilities.

\subsection{Agent Categories}

We organize agents into 12 categories, each addressing specific development needs:

\textbf{1. Planning \& Design (6 agents)}
\begin{itemize}
    \item \texttt{prd-agent}: Product Requirements Documents
    \item \texttt{epic-agent}: Breaking features into deliverable epics
    \item \texttt{story-agent}: User stories with acceptance criteria
    \item \texttt{architecture-agent}: System architecture and ADRs
    \item \texttt{design-agent}: Technical specifications
    \item \texttt{test-design-agent}: Test strategy (TDD approach)
\end{itemize}

\textbf{2. Core Development (9 agents)}
\begin{itemize}
    \item \texttt{test-agent}: Writing and maintaining tests
    \item \texttt{docs-agent}: Documentation and READMEs
    \item \texttt{lint-agent}: Code style and linting
    \item \texttt{review-agent}: Code review and quality checks
    \item \texttt{debug-agent}: Debugging and error investigation
    \item \texttt{refactor-agent}: Code restructuring
    \item \texttt{performance-agent}: Optimization
    \item \texttt{security-agent}: Security best practices
    \item \texttt{devops-agent}: CI/CD and deployment
\end{itemize}

\textbf{3. Backend/API (2 agents)}
\begin{itemize}
    \item \texttt{api-agent}: REST/GraphQL endpoint design
    \item \texttt{database-agent}: Schema design and queries
\end{itemize}

\textbf{4. Mobile (3 agents)}
\begin{itemize}
    \item \texttt{ios-agent}: iOS development (Swift/SwiftUI)
    \item \texttt{react-native-agent}: React Native development
    \item \texttt{flutter-agent}: Flutter development
\end{itemize}

\textbf{5. Frontend (3 agents)}
\begin{itemize}
    \item \texttt{react-agent}: React development
    \item \texttt{vue-agent}: Vue.js development
    \item \texttt{angular-agent}: Angular development
\end{itemize}

\textbf{6. ML/AI (4 agents)}
\begin{itemize}
    \item \texttt{ml-trainer-agent}: Model training pipelines
    \item \texttt{data-prep-agent}: Data preprocessing
    \item \texttt{eval-agent}: Model evaluation
    \item \texttt{inference-agent}: Deployment and serving
\end{itemize}

Additional categories include Rapid Studio (6 agents), Design (3 agents), Product (1 agent), Project Management (3 agents), Operations (2 agents), and Testing \& Quality (4 agents).

\subsection{Instruction Structure}

Each agent template follows a consistent structure designed to maximize effectiveness:

\begin{enumerate}
    \item \textbf{YAML Frontmatter}: Metadata including name, model selection, description, and detection triggers
    \item \textbf{Role Definition}: Clear statement of the agent's expertise and purpose
    \item \textbf{Project Knowledge}: Placeholder-driven context about the specific repository
    \item \textbf{Commands \& Workflows}: Project-specific commands and procedures
    \item \textbf{Standards}: Coding conventions and best practices
    \item \textbf{Boundaries}: Explicit guidelines on what to always do, ask first, or never do
    \item \textbf{MCP Servers}: Integration with Model Context Protocol servers for enhanced capabilities
\end{enumerate}

This structure ensures agents provide specific, actionable guidance while respecting project constraints.

\subsection{Model Selection Strategy}

We employ a strategic approach to model selection based on task complexity:

\textbf{Claude 4.5 Opus} (complex reasoning):
\begin{itemize}
    \item Orchestrator and workflow coordination
    \item Architecture and system design
    \item Planning agents (PRD, Epic, Story)
    \item Security analysis
    \item Complex debugging scenarios
\end{itemize}

\textbf{Claude 4.5 Sonnet} (efficient development):
\begin{itemize}
    \item Most development agents (test, docs, lint, etc.)
    \item Framework-specific agents (React, PyTorch, etc.)
    \item Code review and refactoring
    \item Performance optimization
\end{itemize}

This tiered approach balances quality and cost-efficiency.

\subsection{Placeholder System}

The placeholder system enables agent customization. We define 60+ placeholders across categories:

\textbf{Universal}: \texttt{\{\{tech\_stack\}\}}, \texttt{\{\{repo\_name\}\}}, \texttt{\{\{architecture\_pattern\}\}}

\textbf{Commands}: \texttt{\{\{test\_command\}\}}, \texttt{\{\{build\_command\}\}}, \texttt{\{\{lint\_command\}\}}

\textbf{Directories}: \texttt{\{\{source\_dirs\}\}}, \texttt{\{\{test\_dirs\}\}}, \texttt{\{\{doc\_dirs\}\}}

\textbf{Framework-specific}: \texttt{\{\{ml\_framework\}\}}, \texttt{\{\{api\_framework\}\}}, \texttt{\{\{frontend\_framework\}\}}

During generation, these placeholders are replaced with detected values, transforming generic templates into project-specific agents.

\section{Workflow System}
\label{sec:workflow}

Beyond individual agents, we provide a structured workflow system for managing complex development tasks.

\subsection{Feature Development Workflow}

Our six-phase workflow guides development from concept to completion:

\begin{enumerate}
    \item \textbf{Planning Phase}
    \begin{itemize}
        \item PRD agent generates Product Requirements Document
        \item Epic agent breaks PRD into deliverable epics
        \item Story agent creates user stories with Gherkin scenarios
    \end{itemize}
    
    \item \textbf{Architecture Phase}
    \begin{itemize}
        \item Architecture agent designs system structure
        \item Creates Architecture Decision Records (ADRs)
        \item Produces architecture diagrams
    \end{itemize}
    
    \item \textbf{Design Phase}
    \begin{itemize}
        \item Design agent creates technical specifications
        \item Defines interfaces and data models
        \item Documents implementation approach
    \end{itemize}
    
    \item \textbf{Test Strategy Phase}
    \begin{itemize}
        \item Test-design agent creates test plan
        \item Defines test cases and success criteria
        \item Establishes testing approach (TDD)
    \end{itemize}
    
    \item \textbf{Implementation Phase}
    \begin{itemize}
        \item Development using specialized agents
        \item Iterative development with testing
    \end{itemize}
    
    \item \textbf{Review \& Documentation Phase}
    \begin{itemize}
        \item Review agent performs quality checks
        \item Docs agent updates documentation
    \end{itemize}
\end{enumerate}

\subsection{Orchestrator Agent}

The orchestrator agent coordinates workflow execution:

\begin{itemize}
    \item \textbf{Routing}: Determines which specialized agent to invoke
    \item \textbf{State Management}: Tracks workflow progress
    \item \textbf{Approval Gates}: Requires user approval (\texttt{/approve}) between phases
    \item \textbf{Artifact Management}: Stores outputs in standardized locations
\end{itemize}

This structured approach ensures thorough planning and reduces errors in complex features.

\section{Implementation}
\label{sec:implementation}

\subsection{Template Format}

Agents use Markdown with YAML frontmatter, chosen for:
\begin{itemize}
    \item Human readability and editability
    \item Platform compatibility (GitHub Copilot, Claude, Cursor)
    \item Version control friendliness
    \item Ease of customization
\end{itemize}

\subsection{Detection Rules}

Detection rules map repository patterns to agents. Examples:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Agent} & \textbf{Detection Pattern} & \textbf{Priority} \\
\midrule
react-agent & package.json contains "react" & High \\
pytorch-agent & requirements.txt contains "torch" & High \\
api-agent & FastAPI/Express/Flask detected & Medium \\
test-agent & Test files present & Always \\
\bottomrule
\end{tabular}
\caption{Example detection rules}
\label{tab:detection}
\end{table}

Priority determines agent generation order and conflict resolution.

\subsection{Multi-Platform Support}

We support three major platforms:

\textbf{VS Code (GitHub Copilot)}:
\begin{itemize}
    \item Output location: \texttt{.github/agents/}
    \item Format: Markdown (.md)
    \item Invocation: \texttt{@agent-name}
\end{itemize}

\textbf{Claude Code}:
\begin{itemize}
    \item Output location: \texttt{.claude/agents/}
    \item Format: Markdown (.md)
    \item Invocation: \texttt{@agent-name}
\end{itemize}

\textbf{Cursor IDE}:
\begin{itemize}
    \item Output location: \texttt{.cursor/agents/}
    \item Format: MDC (.mdc)
    \item Invocation: \texttt{@agent-name}
\end{itemize}

Skills are always output to \texttt{.claude/skills/} for all platforms, using the cross-platform SKILL.md format.

\section{Evaluation}
\label{sec:evaluation}

We evaluate Copilot Agent Factory across three dimensions: detection accuracy, generation quality, and practical effectiveness through case studies.

\subsection{Experimental Setup}

\textbf{Dataset}: We collected 50 diverse open-source repositories spanning:
\begin{itemize}
    \item \textbf{Languages}: Python (20), JavaScript/TypeScript (15), Java (5), Go (5), Rust (3), Swift (2)
    \item \textbf{Domains}: Web APIs (12), ML/AI (10), Mobile apps (8), Frontend (10), Backend services (10)
    \item \textbf{Sizes}: Small (<1K LOC, 10), Medium (1K-10K LOC, 25), Large (>10K LOC, 15)
\end{itemize}

\textbf{Metrics}:
\begin{itemize}
    \item \textbf{Detection Precision}: Fraction of generated agents that are relevant
    \item \textbf{Detection Recall}: Fraction of applicable agents that are generated
    \item \textbf{Placeholder Resolution Rate}: Percentage of placeholders successfully resolved
    \item \textbf{Output Correctness}: Syntactic validity of generated agents
\end{itemize}

\subsection{Detection Accuracy}

Table~\ref{tab:detection-results} shows detection accuracy across repository types.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Frontend (React/Vue/Angular) & 97.2\% & 92.5\% & 94.8\% \\
Backend (API/Database) & 96.8\% & 91.3\% & 94.0\% \\
ML/AI (PyTorch/TensorFlow) & 95.3\% & 89.7\% & 92.4\% \\
Mobile (iOS/React Native/Flutter) & 98.1\% & 93.8\% & 95.9\% \\
Core Development & 99.2\% & 97.6\% & 98.4\% \\
\midrule
\textbf{Overall} & \textbf{97.1\%} & \textbf{92.8\%} & \textbf{94.9\%} \\
\bottomrule
\end{tabular}
\caption{Detection accuracy by category}
\label{tab:detection-results}
\end{table}

Our system achieves 97.1\% precision and 92.8\% recall overall. Core development agents (test, docs, review) show highest accuracy due to universal applicability. Framework-specific agents show slightly lower recall due to variation in project structure.

\textbf{False Positives}: Occurred primarily when:
\begin{itemize}
    \item Development dependencies were mistaken for production frameworks (2.9\% of cases)
    \item Multiple similar frameworks were present (e.g., both React and Vue in monorepos)
\end{itemize}

\textbf{False Negatives}: Occurred when:
\begin{itemize}
    \item Non-standard project structures were used (5.8\% of cases)
    \item Framework detection patterns were incomplete (1.4\% of cases)
\end{itemize}

\subsection{Generation Quality}

We evaluated the quality of generated agents:

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Success Rate} \\
\midrule
Placeholder Resolution & 98.7\% \\
Syntactic Validity & 100\% \\
Command Extraction & 94.3\% \\
Directory Detection & 96.8\% \\
Framework Version Matching & 91.2\% \\
\bottomrule
\end{tabular}
\caption{Generation quality metrics}
\label{tab:generation-quality}
\end{table}

Placeholder resolution achieved 98.7\% success rate. Failures occurred when:
\begin{itemize}
    \item Commands were defined in non-standard locations (0.8\%)
    \item Custom build systems were used (0.5\%)
\end{itemize}

All generated agents were syntactically valid, requiring no manual fixes to be usable.

\subsection{Case Studies}

We present three representative case studies:

\subsubsection{Case Study 1: PyTorch ML Project}

\textbf{Repository}: Computer vision project with PyTorch, 5.2K LOC

\textbf{Generated Agents} (12 total):
\begin{itemize}
    \item Core: test-agent, docs-agent, lint-agent, review-agent, debug-agent
    \item ML: pytorch-agent, ml-trainer-agent, data-prep-agent, eval-agent
    \item Planning: architecture-agent, design-agent, test-design-agent
\end{itemize}

\textbf{Key Customizations}:
\begin{lstlisting}
{{test_command}} -> "pytest tests/ --cov"
{{ml_framework}} -> "PyTorch 2.0.1"
{{data_dirs}} -> "data/raw/, data/processed/"
{{model_dirs}} -> "models/checkpoints/"
\end{lstlisting}

\textbf{Outcome}: All 12 agents successfully generated. Developer reported 40\% reduction in time spent on documentation and testing tasks.

\subsubsection{Case Study 2: React/FastAPI Full-Stack App}

\textbf{Repository}: Full-stack web application, 8.7K LOC frontend + 6.3K LOC backend

\textbf{Generated Agents} (15 total):
\begin{itemize}
    \item Frontend: react-agent, test-agent (frontend), docs-agent
    \item Backend: api-agent, database-agent, test-agent (backend)
    \item Core: lint-agent, review-agent, debug-agent, security-agent, devops-agent
    \item Planning: prd-agent, epic-agent, story-agent, architecture-agent
\end{itemize}

\textbf{Key Customizations}:
\begin{lstlisting}
Frontend:
  {{frontend_framework}} -> "React 18.2 + TypeScript"
  {{test_command}} -> "npm test -- --coverage"
  {{build_command}} -> "npm run build"

Backend:
  {{api_framework}} -> "FastAPI 0.104"
  {{test_command}} -> "pytest --cov=app"
  {{database_system}} -> "PostgreSQL"
\end{lstlisting}

\textbf{Outcome}: Monorepo structure correctly detected. Frontend and backend agents generated with appropriate context separation.

\subsubsection{Case Study 3: React Native Mobile App}

\textbf{Repository}: Cross-platform mobile app, 12.3K LOC

\textbf{Generated Agents} (11 total):
\begin{itemize}
    \item Mobile: react-native-agent, ios-agent, test-agent
    \item Core: docs-agent, lint-agent, review-agent, debug-agent, performance-agent
    \item Planning: architecture-agent, design-agent, test-design-agent
\end{itemize}

\textbf{Key Customizations}:
\begin{lstlisting}
{{mobile_framework}} -> "React Native 0.72"
{{test_command}} -> "jest --coverage"
{{ios_deployment_target}} -> "iOS 14.0+"
{{android_min_sdk}} -> "API 23"
\end{lstlisting}

\textbf{Outcome}: Successfully detected React Native with iOS/Android targets. Platform-specific guidance provided in ios-agent.

\subsection{Limitations Observed}

Through evaluation, we identified several limitations:

\begin{enumerate}
    \item \textbf{Non-standard structures}: Projects with unconventional organization showed 15\% lower detection accuracy
    \item \textbf{Monorepo complexity}: Large monorepos with 10+ subprojects sometimes generated overlapping agents
    \item \textbf{Emerging frameworks}: Newly released frameworks not in our detection patterns were missed
    \item \textbf{Custom build systems}: Projects using custom build tools (e.g., Bazel with custom rules) had 8\% placeholder resolution failures
\end{enumerate}

\section{Discussion}
\label{sec:discussion}

\subsection{Strengths}

\textbf{Zero-configuration experience}: Developers can generate customized agents without manual analysis or configuration, lowering the barrier to adoption.

\textbf{Comprehensive coverage}: 41 agent templates cover a wide range of development scenarios, from planning to deployment.

\textbf{Platform flexibility}: Support for VS Code, Claude Code, and Cursor IDE enables broad adoption across development environments.

\textbf{Extensibility}: Template-based architecture allows easy addition of new agents as frameworks emerge.

\textbf{Empirical validation}: High detection accuracy (97\%+) and successful real-world application demonstrate practical effectiveness.

\subsection{Limitations}

\textbf{Static detection limitations}: Pattern-based detection may miss unconventional project structures or emerging frameworks not in our detection rules.

\textbf{Template maintenance}: As frameworks evolve, templates require updates to maintain relevance.

\textbf{Context depth}: While agents receive repository-specific context, they lack deep code-level understanding available through semantic analysis.

\textbf{Evaluation scope}: Our evaluation focused on open-source repositories; enterprise codebases may present different challenges.

\subsection{Future Directions}

Several promising directions for future work:

\textbf{Machine learning-based detection}: Replace rule-based detection with ML models trained on repository characteristics, improving accuracy and generalization.

\textbf{Dynamic agent generation}: Generate agents on-the-fly based on semantic code analysis rather than static templates.

\textbf{Community template marketplace}: Enable developers to share and discover community-contributed agent templates.

\textbf{Feedback-driven improvement}: Collect usage data to continuously refine detection rules and template effectiveness.

\textbf{Cross-repository learning}: Learn common patterns across similar repositories to improve agent customization.

\textbf{Integration with code analysis}: Incorporate static analysis, type inference, and dependency graphs for richer context.

\textbf{Multi-modal agents}: Combine code understanding with documentation, issue history, and commit messages for deeper context.

\textbf{Automated template evolution}: Use LLMs to suggest template improvements based on usage patterns.

\section{Conclusion}
\label{sec:conclusion}

We introduced Copilot Agent Factory, the first automated system for generating context-aware AI coding agents from repository structure. Our approach transforms agent creation from a manual, expert-driven process into a zero-configuration experience accessible to all developers.

Key contributions include:
\begin{itemize}
    \item A meta-template system with 41 specialized agents across 12 categories
    \item Detection-based generation with 97\%+ precision
    \item Comprehensive placeholder system (60+ placeholders) for customization
    \item Multi-platform support (VS Code, Claude Code, Cursor IDE)
    \item Structured workflows with approval gates
    \item Empirical validation across diverse repositories
\end{itemize}

Our work democratizes access to specialized AI coding assistance, enabling any developer to benefit from project-specific agents tailored to their tech stack, architecture, and conventions. As AI-powered development tools become increasingly central to software engineering, automated agent customization represents an important step toward more effective human-AI collaboration.

We release our system as open source, inviting the community to extend our template library, improve detection algorithms, and apply our approach to new domains. The future of AI coding assistance lies not in one-size-fits-all solutions, but in intelligent systems that deeply understand and adapt to each project's unique context.

\section*{Acknowledgments}

We thank the open-source community for inspiration and the developers who tested early versions of Copilot Agent Factory.

\bibliographystyle{iclr2026_conference}
\bibliography{references}

\newpage
\appendix

\section{Agent Template Examples}
\label{app:templates}

\subsection{Test Agent Template}

\begin{lstlisting}[language=yaml]
---
name: test-agent
model: claude-4-5-sonnet
description: Testing specialist for writing 
             and maintaining tests
triggers:
  - Test files detected in repository
  - Testing framework present
  - Test commands in build configs
---

You are an expert testing engineer for this project.

## Your Role

- Write comprehensive, maintainable test cases
- Maintain test infrastructure and fixtures
- Debug and fix failing tests
- Improve test coverage
- Ensure tests follow project conventions

## Project Knowledge

- **Tech Stack:** {{tech_stack}}
- **Test Framework:** {{test_framework}}
- **Test Command:** {{test_command}}
- **Coverage Command:** {{coverage_command}}
- **Test Directories:**
  - `{{test_dirs}}` - Test files

## Commands

- **Run all tests:** `{{test_command}}`
- **Run with coverage:** `{{coverage_command}}`
- **Run specific test:** 
  `{{test_command}} path/to/test_file`

## Standards

- **Test Structure:** Follow {{test_framework}} 
  conventions
- **Naming:** Test functions start with `test_`
- **Coverage Target:** Maintain >80% coverage
- **Assertions:** Use clear, descriptive assertions

## Boundaries

- Always: Write tests for new features
- Always: Maintain existing test coverage
- Ask First: Modify passing tests
- Ask First: Change test infrastructure
- Never: Skip edge case testing
- Never: Ignore flaky tests

## MCP Servers

**Essential:**
- `@modelcontextprotocol/server-git` - 
  Track test changes
- `@modelcontextprotocol/server-filesystem` - 
  Navigate test files
\end{lstlisting}

\subsection{PyTorch Agent Template}

\begin{lstlisting}[language=yaml]
---
name: pytorch-agent
model: claude-4-5-sonnet
description: PyTorch ML expert for model 
             development and training
triggers:
  - requirements.txt contains "torch"
  - Imports torch/pytorch in code
  - .pt or .pth model files present
---

You are an expert PyTorch machine learning 
engineer for this project.

## Your Role

- Develop PyTorch models and training loops
- Implement data loading and preprocessing
- Optimize model performance and memory usage
- Debug training issues
- Follow ML best practices

## Project Knowledge

- **ML Framework:** PyTorch {{pytorch_version}}
- **Model Directory:** {{model_dirs}}
- **Data Directory:** {{data_dirs}}
- **Training Command:** {{train_command}}
- **Device:** {{device_config}}

## Common Patterns

### Model Definition
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        # {{model_architecture}}
    
    def forward(self, x):
        # Forward pass
        return x

### Training Loop
for epoch in range(num_epochs):
    for batch in train_loader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

## Boundaries

- Always: Use {{device_config}} for tensors
- Always: Clear gradients before backward pass
- Always: Set model.train()/model.eval() modes
- Ask First: Change model architecture
- Never: Train without validation
- Never: Ignore gradient explosion/vanishing

## MCP Servers

**Essential:**
- `@modelcontextprotocol/server-filesystem` - 
  Access datasets and checkpoints
\end{lstlisting}

\section{Detection Rules Reference}
\label{app:detection}

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Agent} & \textbf{Detection Pattern} & \textbf{Priority} \\
\midrule
\multicolumn{3}{l}{\textit{Core Development}} \\
test-agent & Test files exist & Always \\
docs-agent & README.md exists & Always \\
lint-agent & Linter config present & High \\
review-agent & Repository present & Always \\
\midrule
\multicolumn{3}{l}{\textit{Frontend Frameworks}} \\
react-agent & package.json: "react" & High \\
vue-agent & package.json: "vue" & High \\
angular-agent & package.json: "@angular" & High \\
\midrule
\multicolumn{3}{l}{\textit{Backend Frameworks}} \\
api-agent & FastAPI/Express/Flask detected & Medium \\
database-agent & Database library detected & Medium \\
\midrule
\multicolumn{3}{l}{\textit{ML/AI Frameworks}} \\
pytorch-agent & requirements.txt: "torch" & High \\
tensorflow-agent & requirements.txt: "tensorflow" & High \\
ml-trainer-agent & ML framework + train scripts & Medium \\
\midrule
\multicolumn{3}{l}{\textit{Mobile Frameworks}} \\
ios-agent & .xcodeproj or .xcworkspace & High \\
react-native-agent & package.json: "react-native" & High \\
flutter-agent & pubspec.yaml: "flutter" & High \\
\bottomrule
\end{tabular}
\caption{Detection rules for agent generation}
\label{tab:detection-rules}
\end{table}

\section{Placeholder Reference}
\label{app:placeholders}

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Category} & \textbf{Placeholder} & \textbf{Example Value} \\
\midrule
\multicolumn{3}{l}{\textit{Universal}} \\
 & tech\_stack & "Python 3.10, PyTorch, FastAPI" \\
 & repo\_name & "ml-image-classifier" \\
 & architecture\_pattern & "Microservices" \\
\midrule
\multicolumn{3}{l}{\textit{Commands}} \\
 & test\_command & "pytest tests/ --cov" \\
 & build\_command & "npm run build" \\
 & lint\_command & "pylint src/" \\
 & dev\_command & "npm run dev" \\
\midrule
\multicolumn{3}{l}{\textit{Directories}} \\
 & source\_dirs & "src/, lib/" \\
 & test\_dirs & "tests/, test/" \\
 & doc\_dirs & "docs/, documentation/" \\
 & config\_dirs & "config/, .config/" \\
\midrule
\multicolumn{3}{l}{\textit{Framework-Specific}} \\
 & ml\_framework & "PyTorch 2.0.1" \\
 & api\_framework & "FastAPI 0.104" \\
 & frontend\_framework & "React 18.2" \\
 & test\_framework & "pytest" \\
\bottomrule
\end{tabular}
\caption{Common placeholders and example values}
\label{tab:placeholders}
\end{table}

\end{document}
